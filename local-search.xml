<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/2025/03/14/CUDA/"/>
    <url>/2025/03/14/CUDA/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2025/03/08/hello-world/"/>
    <url>/2025/03/08/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Machine_learning</title>
    <link href="/2025/03/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <url>/2025/03/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>Field of study that gives computers the ability to learn without being explicitly programmed.</p><h2 id="Machine-learning-algorithms"><a href="#Machine-learning-algorithms" class="headerlink" title="Machine learning algorithms"></a>Machine learning algorithms</h2><ul><li>Supervised learning</li><li>Unsupervised learning </li><li>Recommender systems</li><li>Reinforcement learnin</li></ul><h3 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h3><ul><li>Regression</li><li>Classification<br>Key charateristic:input–&gt;output<br><strong>Regression</strong>:Predicts a number from infinitely many possible numbers.<br><strong>Classification</strong>:Predicts catagories from small number of possible outputs.Categories don’t have to be numbers.<br>When it comes to two or more inputs:The learning algorithm has to decide how to fit a boundary line to this data.</li></ul><h3 id="Unsupervised-learning"><a href="#Unsupervised-learning" class="headerlink" title="Unsupervised learning"></a>Unsupervised learning</h3><p>Find something interesting in unlabeld data.<br>Key charateristc:Data only comes with inputs x,but not output labels y.Algorithm has to find structure in the data.</p><ul><li>Clustering</li><li>Anomaly detection</li><li>Dimensionality reduction<br><strong>Clustering</strong>: Places the unlabeled data into different clusters,automatically finding stucture into data and figuring out what are the major types of individuals.<br><strong>Anomaly detection</strong>:Finds unusual data points.<br><strong>Dimensionality reduction</strong>：Compress data using fewer numbers.</li></ul><hr><h2 id="Jupyter-Notebooks"><a href="#Jupyter-Notebooks" class="headerlink" title="Jupyter Notebooks"></a>Jupyter Notebooks</h2><h1 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h1><p>Training Set–&gt;learing algorithms–&gt;function:model<br>Linear Regressionin with one variable:y&#x3D;wx+b</p><h2 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h2><p>f(x)&#x3D;wx+b w,b:parameters&#x2F;coeffients&#x2F;weights<br><img src="/images/Cost_Function.png" alt="pic" title="Cost Function"><br>Cost Function:Square error cost function<br>Goal:Minimize J(w,b)<br><img src="/images/The_function_of_w.png" alt="pic" title="The function of w"><br><img src="/images/J(w,b).png" alt="pic" title="J(w,b) of Linear Function"></p><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><p>Outline:</p><ul><li>Start with some w,b</li><li>Keep changing w,b to reduce J(w,b)</li><li>Until we settle at or near a minimum</li></ul><h3 id="Implement-Gradient-Descent"><a href="#Implement-Gradient-Descent" class="headerlink" title="Implement Gradient Descent"></a>Implement Gradient Descent</h3><p><img src="/images/Gradient_Descent_Algorithm.png" alt="pic" title="Gradient Descent Algorithm"><br>α：Learing Rate(between 0~1),controls how big of step you take downhill.<br>Derivative:Decide the direction to take step.<br>Repeat until convergence–&gt;reach the point at a local<br>minimum where the parameters w and b no longer change much with each additional step that you take.</p><h3 id="Gradient-Descent-Intuition"><a href="#Gradient-Descent-Intuition" class="headerlink" title="Gradient Descent Intuition"></a>Gradient Descent Intuition</h3><p><img src="/images/Gradient_Descent.png" alt="pic" title="Gradient Descent Intuition"></p><h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><p>If α is too small, Gradient descent may be slow.<br>If α is too large, Gradient descent may:</p><ul><li>Overshoot, never reach minimum</li><li>Fail to converge, diverge<br>Gradient Descent can reach local minimum with fixed learning rate.<br>Near a local minimum,</li><li>Derivative becomes smaller</li><li>Update steps become smaller</li></ul><h3 id="Gradient-Descent-for-Linear-Regression"><a href="#Gradient-Descent-for-Linear-Regression" class="headerlink" title="Gradient Descent for Linear Regression"></a>Gradient Descent for Linear Regression</h3><p><img src="/images/Gradient_Descent_for_Linear_Regression.png" alt="pic" title="Gradient Descent for Linear Regression"><br>“Batch” gradient descent: Each step of gradient descent uses all the training example.</p><h2 id="Linear-Regression-with-Multiple-Variables"><a href="#Linear-Regression-with-Multiple-Variables" class="headerlink" title="Linear Regression with Multiple Variables"></a>Linear Regression with Multiple Variables</h2><p><img src="/images/Multiple_Variables.png" alt="pic" title="Multiple Variables"><br>Models:</p><ul><li>Previously: f(x)&#x3D;wx+b</li><li>Multiple linear regression: f(x)&#x3D; w_1<em>x_1+w_2</em>x_2+w_3<em>x_3+w_4</em>x_4+b<br><img src="/images/Multiple_linear_regression.png" alt="pic" title="Multiple linear regression"></li></ul><h3 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h3><p><img src="/images/Vectorization.png%22" alt="pic" title="Vectorization"> </p><h3 id="Gradient-Descent-for-Multiple-Regression"><a href="#Gradient-Descent-for-Multiple-Regression" class="headerlink" title="Gradient Descent for Multiple Regression"></a>Gradient Descent for Multiple Regression</h3><p><img src="/images/Gradient_Descent_for_Linear_Regression.png" alt="pic" title="Gradient Descent for Multiple Regressionctorization"><br>Normal equation: A alternative to gradient descent.</p><ul><li>Only for linear regression</li><li>Solve for w,b without iterations<br>Disadvantages:</li><li>Doesn’t generalize to other learing algorithms.</li><li>Slow when number of features is large.</li></ul><h2 id="Feature-Scaling"><a href="#Feature-Scaling" class="headerlink" title="Feature Scaling"></a>Feature Scaling</h2><p><img src="/images/Feature_Scaling.png" alt="pic" title="Feature Scaling"></p><ul><li>Mean normalization</li><li>Z-score normalization</li></ul><h3 id="Checking-Gradient-Descent-for-Converge"><a href="#Checking-Gradient-Descent-for-Converge" class="headerlink" title="Checking Gradient Descent for Converge"></a>Checking Gradient Descent for Converge</h3><ul><li>Learning Curve</li><li>Automatic converge test: let epsilon be 0.001. If J(w,b) decreses by ≤ epsilon in one iteration, declare convergence.</li></ul><h3 id="Choosiong-the-Learning-Rate"><a href="#Choosiong-the-Learning-Rate" class="headerlink" title="Choosiong the Learning Rate"></a>Choosiong the Learning Rate</h3><p>With a small enough α, J(w,b) should decrease on every iteration.</p><h2 id="Feature-Engineering"><a href="#Feature-Engineering" class="headerlink" title="Feature Engineering"></a>Feature Engineering</h2><p>Feature Engineering: Using intuition to design new features, by transforming or combining original features.</p><h2 id="Polynomial-Regression"><a href="#Polynomial-Regression" class="headerlink" title="Polynomial Regression"></a>Polynomial Regression</h2><p>Features: x, x squared, x cubed…(Scikit-learn)</p><h1 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h1><ul><li>Negative class</li><li>Positive class</li></ul><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p>Sigmoid function:logistic function<br><img src="/images/Sigmoid_Function.png" alt="pic" title="Sigmoid Function"></p><h3 id="Decision-Boundary"><a href="#Decision-Boundary" class="headerlink" title="Decision Boundary"></a>Decision Boundary</h3><p><img src="/images/Sigmoid_Function_for_Mulitiple_Regression.png" alt="pic" title="Sigmoid Function for Mulitiple Regression"></p><h3 id="Cost-Function-for-Logistic-Regression"><a href="#Cost-Function-for-Logistic-Regression" class="headerlink" title="Cost Function for Logistic Regression"></a>Cost Function for Logistic Regression</h3><p>Squared error cost:</p><ul><li>Linear regression: convex cost function</li><li>Logistic regression: non-convex function<br><img src="/images/Square_error_cost.png" alt="pic" title="Square error cost"><br>Cost Function: average loss over all training samples.<br>Loss Function: predicts the target for single sample </li><li>Target: Measuring the Quality of Predicted Probabilities<br>In a binary classification problem, the model outputs a probability value, which represents the probability that a given sample belongs to class 1.</li><li>Maximum Likelihood Estimation</li><li>Taking the Log to Obtain the Log-Likelihood</li><li>Binary Cross-Entropy Loss<br><img src="/images/Cost_Function_for_Logistic_Regression.png" alt="pic" title="Cost Function for Logistic Regression"><br>When y(i)&#x3D;1:if the model predicts a high probility for class 1, the loss is small; otherwise, the loss is large. When y(i)&#x3D;0, if the model predicts a low probability for class 1 (i.e., a high probability for class 0), the loss is small; otherwise, the loss is large.</li></ul><h3 id="Simplified-Cost-Function"><a href="#Simplified-Cost-Function" class="headerlink" title="Simplified Cost Function"></a>Simplified Cost Function</h3><p><img src="/images/Simplified_Cost_Function.png" alt="pic" title="Simplified Cost Function"></p><h3 id="Gradient-Desctent-Implementation"><a href="#Gradient-Desctent-Implementation" class="headerlink" title="Gradient Desctent Implementation"></a>Gradient Desctent Implementation</h3><p>Goal: Find w,b, try to estimate the probability that the label Y is 1.<br>Same concepts:</p><ul><li>Minitor gradient descent(learning curve)</li><li>Vectorized implementation<br><img src="/images/Gradient_Desctent_Implementation_for_Logistic_Regression.png" alt="pic" title="Gradient Desctent Implementation for Logistic Regression"><br><img src="/images/Difference_between_Logistic_Regression_and_Linear_Regression.png" alt="pic" title="Difference between Logistic Regression and Linear Regression"></li></ul><h2 id="Regularization-to-Reduce-Overfitting"><a href="#Regularization-to-Reduce-Overfitting" class="headerlink" title="Regularization to Reduce Overfitting"></a>Regularization to Reduce Overfitting</h2><h3 id="The-problem-of-overfitting"><a href="#The-problem-of-overfitting" class="headerlink" title="The problem of overfitting"></a>The problem of overfitting</h3><p><img src="/images/Fitting.png" alt="pic" title="Three Regression Examples"></p><ul><li>Does not fit the training set well: high bias–&gt;underfit</li><li>Fits training set pretty well: generalization</li><li>Fits the training set extremely well: high variance–&gt;overfit</li></ul><h3 id="Addresssing-Overfitting"><a href="#Addresssing-Overfitting" class="headerlink" title="Addresssing Overfitting"></a>Addresssing Overfitting</h3><ul><li>Collect more training data</li><li>Select features to include&#x2F;exclude</li><li>Regularization: encourage the learning algorithm to shrink the values of parameters without neccessarily demanding that the parameter is set to exactly zero.</li></ul><h4 id="Cost-Function-with-Regularization"><a href="#Cost-Function-with-Regularization" class="headerlink" title="Cost Function with Regularization"></a>Cost Function with Regularization</h4><p>Lambda: regularization parameter<br>If lambda is too enormous–&gt;underfit<br>If lambda is too small–&gt;overfit<br><img src="/images/Regularization.png" alt="pic" title="Cost Function with Regularization"></p><h4 id="Regularized-Linear-Regression"><a href="#Regularized-Linear-Regression" class="headerlink" title="Regularized Linear Regression"></a>Regularized Linear Regression</h4><p><img src="/images/Regularized_Linear_Regression.png" alt="pic" title="Cost Function with Regularization"></p><h4 id="Regularuzed-Logistic-Regression"><a href="#Regularuzed-Logistic-Regression" class="headerlink" title="Regularuzed Logistic Regression"></a>Regularuzed Logistic Regression</h4><p><img src="/images/Regularized_Logistic_Regression.png" alt="pic" title="Regularuzed Logistic Regression"></p><h1 id="Advanced-learning-algorithms"><a href="#Advanced-learning-algorithms" class="headerlink" title="Advanced learning algorithms"></a>Advanced learning algorithms</h1><h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><p>Simplified mathematical model of neuron: inputs–&gt;neuron–&gt;outputs</p><h3 id="inference-prediction"><a href="#inference-prediction" class="headerlink" title="inference(prediction)"></a>inference(prediction)</h3><p><img src="/images/Demand_Prediction.png" alt="pic" title="Demand Prediction"></p><ul><li>Layer: A group of neurons which take as input the same or similiar features and that in turn output a few numbers together.</li><li>Affordability, awareness, perceived quality–&gt;<strong>activations</strong><br><img src="/images/Layer.png" alt="pic" title="Layer"></li><li>input layer: 4 numbers(vector)–&gt;hidden layer: 3 numbers–&gt;ouput layer: 1 numbers(vector)</li><li>Why called hidden layer: The data set tells you what is x and what is y, so you get data that tells you what are the correct inputs and the correct outputs, but the data set doesn’t tell you what are the correct values for affordability, awareness, perceived quality, so the correct data is hidden in the training set.</li><li>Cover up the left half of this diagram: a  logistic regression algorithm that is taking as input affordability, awareness, perceived quality of a t-shirt and using these three features to estimate the probability of the t-shirt being a top seller. What the neural network does is instead of you needing  to manually engineer the features, it can learn.</li><li>Multiple hidden layers(Multilayer perception): neural network architecture<br><img src="/images/Multiple_hidden_layers.png" alt="pic" title="Multiple hidden layers"></li></ul><h4 id="Example-Recognizing-Images"><a href="#Example-Recognizing-Images" class="headerlink" title="Example: Recognizing Images"></a>Example: Recognizing Images</h4><ul><li>Goal: Train a network with a million pixel brightness values, and outputs the indentity of the person in the picture.</li><li>Hidden layer: the 1st:looking for a little vertical line or edge; a oriented line; a line that orientation…the 2nd: group together; the 3rd: face </li><li>A remarkable feature: the neural network can learn these features detectors at the hidden layers all by itself.</li></ul><h4 id="Neural-network-layer"><a href="#Neural-network-layer" class="headerlink" title="Neural network layer"></a>Neural network layer</h4><p><img src="/images/Neural_network_layer.png" alt="pic" title="Neural network layer"> </p><h4 id="More-complex-neural-networks"><a href="#More-complex-neural-networks" class="headerlink" title="More complex neural networks"></a>More complex neural networks</h4><p><img src="/images/general_form.png" alt="pic" title="General Form"><br>j: jth neuron<br>j: jth neuron<br>[l]: lth layer<br>g: sigmoid function–&gt;<strong>activation function</strong><br>Each unit is a single neuron in the layer</p><h4 id="Inference-making-predictions-forward-propagation"><a href="#Inference-making-predictions-forward-propagation" class="headerlink" title="Inference: making predictions(forward propagation)"></a>Inference: making predictions(forward propagation)</h4><p>A handwritten digit recognition sample:<br>Forward  propagation: computation goes from left to right, propagating the activations of the neurons. These computations in the forward direction and this is in constrast to a different algorithm called backword propagation or back propagation.</p><h4 id="Building-the-model-using-TensorFlow"><a href="#Building-the-model-using-TensorFlow" class="headerlink" title="Building the model using TensorFlow"></a>Building the model using TensorFlow</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">x=np.array([[<span class="hljs-number">200.0</span>,<span class="hljs-number">17.0</span>]])<br>layer_1= Dense(units=<span class="hljs-number">3</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>a1= layer_1(x)<span class="hljs-comment">#1*3 matrix</span><br></code></pre></td></tr></table></figure><ul><li>Data in TensorFlow:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">x=np.array([[<span class="hljs-number">200</span>,<span class="hljs-number">17</span>]])<span class="hljs-comment">#[200,17] 1*2</span><br>x=np.array([[<span class="hljs-number">200</span>],<br>            [<span class="hljs-number">17</span>]<br>               ]) <span class="hljs-comment">#2*1--&gt;TensorFloW</span><br>x=np.array([<span class="hljs-number">200</span>,<span class="hljs-number">17</span>])<span class="hljs-comment">#1d vector, a linear array with no rows or no columns.--&gt; Linear Regression, Logistic Regression</span><br>tf.Tensor([[<span class="hljs-number">0.2</span> <span class="hljs-number">0.7</span> <span class="hljs-number">0.3</span>]], shape=(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>), dtype=float32)<br><br>layer_2= Dense(units=<span class="hljs-number">3</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>a2= layer_2(a1)<br>tf.Tensor([[<span class="hljs-number">0.8</span>]], shape=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), dtype=float32)<br>a2.numpy()<br></code></pre></td></tr></table></figure><h4 id="Building-a-neural-network"><a href="#Building-a-neural-network" class="headerlink" title="Building a neural network"></a>Building a neural network</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">layer_1= Dense(units=<span class="hljs-number">3</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>layer_2= Dense(units=<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>model = Sequential([layer_1, layer_2])<br>x = np.array([[<span class="hljs-number">200</span>,<span class="hljs-number">17</span>],<br>              [<span class="hljs-number">120</span>,<span class="hljs-number">5</span>]<br>              [<span class="hljs-number">425</span>,<span class="hljs-number">20</span>] <br>              [<span class="hljs-number">212</span>,<span class="hljs-number">18</span>]])<br>y = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>])<br>model.<span class="hljs-built_in">compile</span>(...)<br>model.fit(x,y)<br>model.predict(x_new)<span class="hljs-comment">#ouput a2</span><br><br>model = Sequential([<br>    Dense(units=<span class="hljs-number">3</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>    Dense(units=<span class="hljs-number">1</span> activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)])   <br></code></pre></td></tr></table></figure><h4 id="Forword-prop-in-a-single-layer"><a href="#Forword-prop-in-a-single-layer" class="headerlink" title="Forword prop in a single layer"></a>Forword prop in a single layer</h4><p><img src="/images/Forword_Prop.png" alt="pic" title="Forword Prop"> </p><h4 id="General-implemention-of-forward-propagation"><a href="#General-implemention-of-forward-propagation" class="headerlink" title="General implemention of forward propagation"></a>General implemention of forward propagation</h4><p><img src="/images/General_implemention_of_forward_propagation.png" alt="pic" title="General implemention of forward propagation"></p><h4 id="Is-there-a-path-to-AGI"><a href="#Is-there-a-path-to-AGI" class="headerlink" title="Is there a path to AGI"></a>Is there a path to AGI</h4><ul><li>ANI：artificial narrow intelligence</li><li>AGI: artificial general intelligence</li></ul><h4 id="Vectorization-1"><a href="#Vectorization-1" class="headerlink" title="Vectorization"></a>Vectorization</h4><p><img src="/images/Loops_vs_Vetetorization.png" alt="pic" title="Loops vs Vetetorization"> </p><h5 id="Matrix-multiplication"><a href="#Matrix-multiplication" class="headerlink" title="Matrix multiplication"></a>Matrix multiplication</h5><p><img src="/images/Matrix_multiplication_in_numpy.png" alt="pic" title="Matrix multiplication in Numpy"><br><img src="/images/Matrix_multiplication_in_tensorflow.png" alt="pic" title="Matrix Multiplication in Tensorflow"></p><h3 id="Net-Network-Training"><a href="#Net-Network-Training" class="headerlink" title="Net Network Training"></a>Net Network Training</h3><h4 id="Training-Details"><a href="#Training-Details" class="headerlink" title="Training Details"></a>Training Details</h4><ul><li>specify how to compute output given input x and parameters w,b(define model)–&gt;f(x)&#x3D;?</li><li>specify loss and cost </li><li>train on data to minimiaze J(w,b)<br><img src="/images/Model_Training_Steps.png" alt="pic" title="Model Training Steps"></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf <br><span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> Sequential<br><span class="hljs-keyword">from</span> tensorflow,keras <span class="hljs-keyword">import</span> Dense<br>    model = Sequential([<br>    Dense(units=<span class="hljs-number">25</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>),<br>    Dense(units=<span class="hljs-number">15</span> activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>),<br>    Dense(units=<span class="hljs-number">1</span> activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>),<br>                      ]) <br><span class="hljs-keyword">from</span> tensorflow.keras.losses <span class="hljs-keyword">import</span><br>BinaryCrossentropy<br>    model.<span class="hljs-built_in">compile</span>(loss=BinaryCrossentropy())<br><br>    model.fit(X,Y.epochs=<span class="hljs-number">100</span>)<span class="hljs-comment">#number of steps in gradient descent--&gt;compute derivatives for gradient descent using &quot;back propagation&quot;</span><br></code></pre></td></tr></table></figure><h4 id="Alternatives-to-the-sigmoid-activation"><a href="#Alternatives-to-the-sigmoid-activation" class="headerlink" title="Alternatives to the sigmoid activation"></a>Alternatives to the sigmoid activation</h4><p>ReLU: g(z)&#x3D;max(0,z)<br>Linear activation function: g(z)&#x3D;z</p><h4 id="Choosing-activation-function"><a href="#Choosing-activation-function" class="headerlink" title="Choosing activation function"></a>Choosing activation function</h4><ul><li>Output Layer:<br>Binary classification: Sigmoid<br>Regression: Linear(can be negative or positive)<br>Regression: ReLU(only positive)</li><li>Hidden Layer:<br>Most common choice: ReLU –&gt;faster<br>Binary classification: Sigmoid–&gt;flat, slow down learning</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tensorflow,keras <span class="hljs-keyword">import</span> Dense<br>model = Sequential([<br>    Dense(units=<span class="hljs-number">25</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    Dense(units=<span class="hljs-number">15</span> activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    Dense(units=<span class="hljs-number">1</span> activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>),<br>                      ]) <br></code></pre></td></tr></table></figure><h4 id="Why-do-we-need-activation-functions"><a href="#Why-do-we-need-activation-functions" class="headerlink" title="Why do we need activation functions?"></a>Why do we need activation functions?</h4><p>Don’t use linear activations in hidden layers</p><h3 id="Multiclass"><a href="#Multiclass" class="headerlink" title="Multiclass"></a>Multiclass</h3><h4 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h4><p>Multiclass classification problem: target y can take on more than two possible values</p><ul><li>Softmax:<br><img src="/images/Softmax.png" alt="pic" title="Softmax"></li><li>Cost:<br><img src="/images/Cost.png" alt="pic" title="Cost"></li></ul><h4 id="Neural-Network-with-Softmax-output"><a href="#Neural-Network-with-Softmax-output" class="headerlink" title="Neural Network with Softmax output"></a>Neural Network with Softmax output</h4><p><img src="/images/Neural_Network_with_Softmax_output.png" alt="pic" title="Neural Network with Softmax output"></p><ul><li>Property: Each of these activation values depends onn all of the values of z.</li><li>Sparse: each digit is th e only one of these catagories.<br><img src="/images/MNIST.png" alt="pic" title="MNIST with softmax"></li></ul><h4 id="Improved-implementation-of-softmax"><a href="#Improved-implementation-of-softmax" class="headerlink" title="Improved implementation of softmax"></a>Improved implementation of softmax</h4><ul><li>Logistic regression:<br><img src="/images/Improved_implementation_of_Logistic_regression.png" alt="pic" title="Improved implementation of Logistic regression"></li><li>Softmax regression:<br><img src="/images/Improved_implementation_of_Softmax_regression.png" alt="pic" title="Improved implementation of Softmax regression"></li><li>Output layer use linear activation function: output z1~z10</li></ul><h4 id="Classification-with-multiple-outputs"><a href="#Classification-with-multiple-outputs" class="headerlink" title="Classification with multiple outputs"></a>Classification with multiple outputs</h4><p><img src="/images/Multilabel_Classification.png" alt="pic" title="Multilabel Classification"></p><h3 id="Advanced-Optimization"><a href="#Advanced-Optimization" class="headerlink" title="Advanced Optimization"></a>Advanced Optimization</h3><ul><li>Adam algotithm: adjust α automatically<br><img src="/images/Adam.png" alt="pic" title="MNIST Adam "></li></ul><h3 id="Addictional-Layer-Type"><a href="#Addictional-Layer-Type" class="headerlink" title="Addictional Layer Type"></a>Addictional Layer Type</h3><ul><li>Dense layer: every neuron in a layer gets as its inputs all the activations from the previous layer</li><li>Convolutional layer: Each neuron only looks at part of the previous layer’s outputs.<br>Why?</li><li>Faster computation</li><li>Need less training data(less prone to overfitting)<br><img src="/images/Convolutional_Neural_Network.png" alt="pic" title="Convolutional Neural Network"></li></ul><h2 id="Pratical-advice-for-building-machine-learning-systems"><a href="#Pratical-advice-for-building-machine-learning-systems" class="headerlink" title="Pratical advice for building machine learning systems"></a>Pratical advice for building machine learning systems</h2><h3 id="Debugging-a-learning-algorithm"><a href="#Debugging-a-learning-algorithm" class="headerlink" title="Debugging a learning algorithm"></a>Debugging a learning algorithm</h3><p>When it makes unacceptably large errors in predictions.</p><ul><li>Get more training examples</li><li>Try smaller sets of features </li><li>Try getting additional features</li><li>Try adding polynomial features</li><li>Try decreasing lambda </li><li>Try increasing lambda<br>Diagnostic: A test that you run to gain insight into what is&#x2F;isn’t working with a learning algorithm, to gain guidance into improving its performance.</li></ul><h3 id="Evaluating-a-model"><a href="#Evaluating-a-model" class="headerlink" title="Evaluating a model"></a>Evaluating a model</h3><ul><li>Training Set</li><li>Test Set<br><img src="/images/Train/test_procedure_for_linear_regression.png" alt="pic" title="Train&#x2F;test procedure for linear regression(with squared error cost"><br><img src="/images/Train/test_procedure_for_classification_problem.png" alt="pic" title="Train&#x2F;test procedure for classification problem(with squared error cost"></li><li>fraction of the test set and the fraction of  the train set that the algorithm has misclassified–&gt;count y_hat ! &#x3D; y</li><li>J_test(w,b): the fraction of the test set that has been misclassified</li><li>J_train(w,b): the fraction of the train set that has been misclassified</li></ul><h4 id="Model-selection-and-training-cross-validation-test-sets"><a href="#Model-selection-and-training-cross-validation-test-sets" class="headerlink" title="Model selection and training&#x2F;cross validation&#x2F;test sets"></a>Model selection and training&#x2F;cross validation&#x2F;test sets</h4><p><img src="/images/Train/Model_Selection.png" alt="pic" title="Train&#x2F;Model Selection"><br><img src="/images/Train/Cross_validation.png" alt="pic" title="Training&#x2F;cross validation&#x2F;test set"></p><h4 id="Diagnosing-bias-and-variance"><a href="#Diagnosing-bias-and-variance" class="headerlink" title="Diagnosing bias and variance"></a>Diagnosing bias and variance</h4><h2 id="Decision-Trees"><a href="#Decision-Trees" class="headerlink" title="Decision Trees"></a>Decision Trees</h2>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
