<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Heterogeneous Parallel Computing</title>
    <link href="/2025/03/14/CUDA/"/>
    <url>/2025/03/14/CUDA/</url>
    
    <content type="html"><![CDATA[<h1 id="FDTD"><a href="#FDTD" class="headerlink" title="FDTD"></a>FDTD</h1><h2 id="calculate-on-a-GPU"><a href="#calculate-on-a-GPU" class="headerlink" title="calculate on a GPU"></a>calculate on a GPU</h2><h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><table><thead><tr><th>main.cu</th><th>kernel.u</th><th>kenel.cuh</th></tr></thead><tbody><tr><td>malloc memory、initialize variable、call the functions、free memory</td><td>calculate ez in one GPU</td><td>declare kernel function</td></tr></tbody></table><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs cuda">main.cu<br>    main(): <br>        define ntimestep = 200, row = 256 ,column =256<br>        caculate mem_size = sizeof(float) * row * coluumn <br>        malloc memory for variable *h_hx1d, *h_hy1d, *h_ez1d<br>        use for loop to malloc dynamic memory for variable *h_hx2d, *h_hy2d, *h_ez2d<br>        use **cudaMalloc** to malloc memory for d_hx, d_hy, d_ez which size = mem_size<br>        use a for loop to initialize h_hx1d, h_hy1d, h_ez1d<br>        use cudaMemcpy to copy the host memory to device<br>        difine grid(16, 16, 1) --&gt; gridDim.x = gridDim .y =16<br>        difine threads(16, 16, 1) --&gt; blockDim.x = blockDim .y =16<br>        call the iteration function<br>        use cudaMemcpy to copy the device memory to host<br>        use a for loop to assign the data from the 1D array ez1d to the 2D array ez2d  <br>        free host memory and device memory  <br>    cpuiteration():<br>        Define source position: isource = 16*8+8, jsource = 16*8+8<br>        Dynamically allocate memory for hx, hy, ez<br>        Use nested loops to set boundary conditions<br>        Calculate electromagnetic fields according to FDTD equations<br>        Output results to the cpuez.dat file <br></code></pre></td></tr></table></figure><h4 id="CUDA-Kernel-iteration-Execution-Process-Analysis"><a href="#CUDA-Kernel-iteration-Execution-Process-Analysis" class="headerlink" title="CUDA Kernel iteration Execution Process Analysis"></a>CUDA Kernel <code>iteration</code> Execution Process Analysis</h4><h5 id="Initialization-and-Parameter-Setup"><a href="#Initialization-and-Parameter-Setup" class="headerlink" title="Initialization and Parameter Setup"></a>Initialization and Parameter Setup</h5><p>First, the kernel defines several physical constants and grid parameters:</p><ul><li>Physical constants: π, speed of light c, wavelength, etc.</li><li>Spatial and temporal steps: dx and dt</li><li>Angular frequency omega</li></ul><h5 id="Thread-Index-Calculation"><a href="#Thread-Index-Calculation" class="headerlink" title="Thread Index Calculation"></a>Thread Index Calculation</h5><p>Each thread calculates its position in the entire computational domain:</p><ul><li>Using blockIdx and threadIdx to determine the thread’s position in the 2D grid</li><li>Computing a unique global index <code>kn</code> for accessing array elements in global memory</li><li>Defining the source position <code>source</code></li></ul><h5 id="Computation-Execution"><a href="#Computation-Execution" class="headerlink" title="Computation Execution"></a>Computation Execution</h5><p>The kernel performs two different calculations based on the <code>type</code> parameter:</p><h6 id="When-type-0-Update-Electric-Field-Ez"><a href="#When-type-0-Update-Electric-Field-Ez" class="headerlink" title="When type&#x3D;0 (Update Electric Field Ez):"></a>When type&#x3D;0 (Update Electric Field Ez):</h6><ol><li><p><strong>Boundary Condition Handling</strong>:</p><ul><li>If the thread is located at the boundary of the computational domain, Ez is set to 0</li></ul></li><li><p><strong>Source Processing</strong>:</p><ul><li>If the current position is the source position, Ez is set to a sine wave: <code>sin(omega*n*dt)</code></li></ul></li><li><p><strong>FDTD Update</strong>:</p><ul><li>For other positions, Ez is updated according to the FDTD equation:</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">Ez<span class="hljs-selector-attr">[kn]</span> += <span class="hljs-number">0.5</span>*((Hy<span class="hljs-selector-attr">[kn]</span>-Hy<span class="hljs-selector-attr">[kn-1]</span>)<span class="hljs-built_in">-</span>(Hx<span class="hljs-selector-attr">[kn]</span>-Hx<span class="hljs-selector-attr">[kn-Dim]</span>))<br></code></pre></td></tr></table></figure><ul><li>This implements the update of the electric field Ez based on spatial derivatives of the magnetic fields Hx and Hy</li></ul></li></ol><h6 id="When-type-1-Update-Magnetic-Fields-Hx-and-Hy"><a href="#When-type-1-Update-Magnetic-Fields-Hx-and-Hy" class="headerlink" title="When type&#x3D;1 (Update Magnetic Fields Hx and Hy):"></a>When type&#x3D;1 (Update Magnetic Fields Hx and Hy):</h6><ol><li><p><strong>Update Hx</strong>:</p><ul><li>If not at the upper boundary in the y direction, Hx is updated based on the spatial derivative of Ez:</li></ul><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs inform7">Hx<span class="hljs-comment">[kn]</span> -= 0.5*(Ez<span class="hljs-comment">[kn+Dim]</span>-Ez<span class="hljs-comment">[kn]</span>)<br></code></pre></td></tr></table></figure><ul><li>Otherwise, Hx is set to 0 at the boundary</li></ul></li><li><p><strong>Update Hy</strong>:</p><ul><li>If not at the right boundary in the x direction, Hy is updated based on the spatial derivative of Ez:</li></ul><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Hy</span>[kn] += <span class="hljs-number">0</span>.<span class="hljs-number">5</span>*(Ez[kn+<span class="hljs-number">1</span>]-Ez[kn])<br></code></pre></td></tr></table></figure><ul><li>Otherwise, Hy is set to 0 at the boundary</li></ul></li></ol><h3 id="questions"><a href="#questions" class="headerlink" title="questions"></a>questions</h3><h2 id="questions-1"><a href="#questions-1" class="headerlink" title="questions"></a>questions</h2><ul><li>可不可以写成<code>float* h_hx1d =malloc(mem_size)</code>;没有<code>float*</code>会怎么样</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-type">float</span>* h_hx1d =(<span class="hljs-type">float</span>*)<span class="hljs-built_in">malloc</span>(mem_size);<br></code></pre></td></tr></table></figure><p>在C语言中，<code>malloc</code>函数用于动态分配内存，并返回一个指向分配内存的指针。它的返回值是<code>void*</code>类型，这意味着它是一个通用指针，必须显式地转换为适当的指针类型才能正确使用。</p><ul><li>为什么分配二维数组时，行可以像一维直接分配，列却需要使用循环</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs cuda"><br>float ** h_hx2d=(float **)malloc(row*sizeof(float*) );<br>float ** h_hy2d=(float **)malloc(row*sizeof(float*) );<br>float ** h_ez2d=(float **)malloc(row*sizeof(float*) );<br>for (i=0;i&lt;row;i++)&#123;<br>    h_hx2d[i]=(float*)malloc(column*sizeof(float));<br>    h_hy2d[i]=(float*)malloc(column*sizeof(float));<br>    h_ez2d[i]=(float*)malloc(column*sizeof(float));<br>&#125;<br></code></pre></td></tr></table></figure><p>在C语言中，二维数组的内存分配与一维数组有些不同，因为C语言本身不直接支持动态的二维数组。为了实现动态的二维数组，通常需要分两步进行内存分配：首先为行指针数组分配内存，然后为每一行分配内存。</p><ul><li>grid, theards参数定义</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cuda">dim3 grid(16,16,1);<br>dim3 threads(16,16,1);<br></code></pre></td></tr></table></figure><p>在CUDA编程中，<code>dim3</code>类型用于定义网格（grid）和线程块（block）的维度。CUDA通过将计算任务分配到多个线程中并行执行来加速计算，而这些线程被组织成线程块和网格。</p><ul><li>核函数调用</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs cuda"><br>for (i=0;i&lt;ntimestep;i++) &#123;<br>    iteration &lt;&lt;&lt; grid, threads &gt;&gt;&gt;(d_hx,d_hy,d_ez,i,0);<br>    iteration &lt;&lt;&lt; grid, threads &gt;&gt;&gt;(d_hx,d_hy,d_ez,i,1);<br>    cudaThreadSynchronize();<br>&#125;<br></code></pre></td></tr></table></figure><ul><li>为什么要循环多次调用iteration</li></ul><ol><li>时间步进：在电磁波传播模拟中，电场和磁场的状态需要在每个时间步上进行更新。每次迭代对应于模拟中的一个时间步。</li><li>逐步更新场变量：电场（<code>g_ez</code>）和磁场（<code>g_hx</code>和<code>g_hy</code>）的值需要逐步更新，以模拟电磁波在空间中的传播。</li></ol><ul><li>这个函数中，kn是一个已知的定值吗？<br><code>kn</code> 不是一个定值，而是一个动态计算的索引值，用于确定当前 CUDA 线程应该处理的数组元素位置。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cuda">unsigned int kn = gridDim.x * blockDim.x * blockDim.y * blockIdx.y   <br>    + gridDim.x * blockDim.x * threadIdx.y<br>    + blockDim.x * blockIdx.x<br>    + threadIdx.x;<br></code></pre></td></tr></table></figure><p>这个计算公式将二维网格中的线程位置映射到一维数组索引。</p><ol><li><p>每个线程执行时，都会基于自己的位置信息计算出唯一的 <code>kn</code> 值：</p><ul><li><code>blockIdx.x</code>, <code>blockIdx.y</code>: 当前线程块在网格中的坐标</li><li><code>threadIdx.x</code>, <code>threadIdx.y</code>: 当前线程在块内的坐标</li><li><code>gridDim.x</code>: 网格在 x 方向的块数量</li><li><code>blockDim.x</code>, <code>blockDim.y</code>: 每个块在 x 和 y 方向的线程数量</li></ul></li><li><p>每个线程执行时会得到不同的 <code>kn</code> 值，对应于它负责处理的网格点</p></li><li><p>这种计算方式实际上是将二维网格的坐标 (x, y) 映射到一维数组的索引，使得每个线程可以访问自己负责的数组元素</p></li></ol><p>因此，<code>kn</code> 是一个变量，其值取决于执行该代码的特定线程在 CUDA 网格中的位置。每个线程都会计算出自己独特的 <code>kn</code> 值，用于访问全局内存中的相应数据元素。</p><ul><li>比较两种索引方法</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs cuda">unsigned int Dim=gridDim.x*blockDim.x;<br>unsigned int kn =gridDim.x * blockDim.x * blockDim.y * blockIdx.y   <br>    + gridDim.x*blockDim.x * threadIdx.y<br>    + blockDim.x * blockIdx.x<br>    + threadIdx.x;    <br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cuda">int col = blockIdx.x*blockDim.x + threadIdx.x;<br>int row = blockIdx.y*blockDim.y + threadIdx.y;<br>int index = row*width + col;<br></code></pre></td></tr></table></figure><ol><li><p>表达方式不同</p><ul><li>第一种方法直接用一个公式计算</li><li>第二种方法分步计算，先计算全局的行列索引，再转换为一维索引</li></ul></li><li><p>width 参数的含义</p><ul><li>在第二种方法中，<code>width</code> 表示二维网格在 x 方向（列方向）的总宽度</li><li>在第一种方法中，这个值隐含为 <code>gridDim.x * blockDim.x</code></li></ul></li><li><p>假设条件</p><ul><li>第一种方法假设网格在 x 方向的总宽度就是 <code>gridDim.x * blockDim.x</code></li><li>第二种方法中的 <code>width</code> 可能是任意值（通常是数据的实际宽度）</li></ul></li><li><p>等价性证明<br>如果 <code>width = gridDim.x * blockDim.x</code>，那么两种方法是完全等价的：</p></li></ol><ul><li>时间步200，每个i都会执行65536个线程吗？</li></ul><ol><li>循环执行 200 次（<code>i</code> 从 0 到 199）</li><li>每次循环中：<ul><li>第一次核函数调用启动 65,536 个线程（更新电场）</li><li>第二次核函数调用也启动 65,536 个线程（更新磁场）</li><li><code>cudaThreadSynchronize()</code> 确保所有线程完成执行后再进入下一个时间步</li></ul></li></ol><ul><li>kernal边界条件</li></ul><ol><li><p>电场为零的条件：</p><ul><li>在理想导体的表面，电场的切向分量必须为零。这是因为任何非零的切向电场分量都会在导体中感应出电流，直到电场消失。</li><li>在代码中，通过将电场 <code>g_ez[kn]</code> 设置为零，确保了在仿真区域的边界上满足PEC条件。</li></ul></li><li><p>边界线程的处理：</p><ul><li>代码中条件 <code>((bx==0)&amp;&amp;(tx==0))||((bx==(gridDim.x-1))&amp;&amp;(tx==(blockDim.x-1)))||((by==0)&amp;&amp;(ty==0))||((by==(gridDim.y-1))&amp;&amp;(ty==(blockDim.y-1)))</code> 检查的是线程是否位于仿真区域的边界。</li><li>如果线程位于边界位置，则将对应的电场值 <code>g_ez[kn]</code> 设置为零，模拟理想导体的反射特性。</li></ul></li></ol><ul><li>cpuiteration边界条件</li></ul><ol><li>电场Ez的更新</li></ol><ul><li>循环范围是 <code>i=1</code> 到 <code>i=row-2</code>，<code>j=1</code> 到 <code>j=column-2</code></li><li>这意味着边界上的电场值 <code>ez[0][j]</code>, <code>ez[row-1][j]</code>, <code>ez[i][0]</code>, <code>ez[i][column-1]</code> 不会被更新</li><li>这些边界值保持为初始值0，相当于实现了一个固定边界条件（Dirichlet边界条件）</li></ul><ol start="2"><li>磁场Hx的更新</li></ol><ul><li>循环范围是 <code>i=0</code> 到 <code>i=row-1</code>，<code>j=0</code> 到 <code>j=column-2</code></li><li>所有行的Hx都被更新，但最后一列的Hx不更新</li><li>这是因为Hx在x方向上与Ez交错排列，最后一列的Hx不需要计算</li></ul><ol start="3"><li>磁场Hy的更新</li></ol><ul><li>循环范围是 <code>i=0</code> 到 <code>i=row-2</code>，<code>j=0</code> 到 <code>j=column-1</code></li><li>所有列的Hy都被更新，但最后一行的Hy不更新</li><li>这是因为Hy在y方向上与Ez交错排列，最后一行的Hy不需要计算</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2025/03/08/hello-world/"/>
    <url>/2025/03/08/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Machine_learning</title>
    <link href="/2025/03/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <url>/2025/03/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>Field of study that gives computers the ability to learn without being explicitly programmed.</p><h2 id="Machine-learning-algorithms"><a href="#Machine-learning-algorithms" class="headerlink" title="Machine learning algorithms"></a>Machine learning algorithms</h2><ul><li>Supervised learning</li><li>Unsupervised learning </li><li>Recommender systems</li><li>Reinforcement learning</li></ul><h3 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h3><ul><li>Regression</li><li>Classification<br>Key charateristic:input–&gt;output<br><strong>Regression</strong>:Predicts a number from infinitely many possible numbers.<br><strong>Classification</strong>:Predicts catagories from small number of possible outputs.Categories don’t have to be numbers.<br>When it comes to two or more inputs:The learning algorithm has to decide how to fit a boundary line to this data.</li></ul><h3 id="Unsupervised-learning"><a href="#Unsupervised-learning" class="headerlink" title="Unsupervised learning"></a>Unsupervised learning</h3><p>Find something interesting in unlabeld data.<br>Key charateristc:Data only comes with inputs x,but not output labels y.Algorithm has to find structure in the data.</p><ul><li>Clustering</li><li>Anomaly detection</li><li>Dimensionality reduction<br><strong>Clustering</strong>: Places the unlabeled data into different clusters,automatically finding stucture into data and figuring out what are the major types of individuals.<br><strong>Anomaly detection</strong>:Finds unusual data points.<br><strong>Dimensionality reduction</strong>：Compress data using fewer numbers.</li></ul><hr><h2 id="Jupyter-Notebooks"><a href="#Jupyter-Notebooks" class="headerlink" title="Jupyter Notebooks"></a>Jupyter Notebooks</h2><h1 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h1><p>Training Set–&gt;learing algorithms–&gt;function:model<br>Linear Regressionin with one variable:y&#x3D;wx+b</p><h2 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h2><p>f(x)&#x3D;wx+b w,b:parameters&#x2F;coeffients&#x2F;weights<br><img src="/images/Cost_Function.png" alt="pic" title="Cost Function"><br>Cost Function:Square error cost function<br>Goal:Minimize J(w,b)<br><img src="/images/The_function_of_w.png" alt="pic" title="The function of w"><br><img src="/images/J(w,b).png" alt="pic" title="J(w,b) of Linear Function"></p><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><p>Outline:</p><ul><li>Start with some w,b</li><li>Keep changing w,b to reduce J(w,b)</li><li>Until we settle at or near a minimum</li></ul><h3 id="Implement-Gradient-Descent"><a href="#Implement-Gradient-Descent" class="headerlink" title="Implement Gradient Descent"></a>Implement Gradient Descent</h3><p><img src="/images/Gradient_Descent_Algorithm.png" alt="pic" title="Gradient Descent Algorithm"><br>α：Learing Rate(between 0~1),controls how big of step you take downhill.<br>Derivative:Decide the direction to take step.<br>Repeat until convergence–&gt;reach the point at a local<br>minimum where the parameters w and b no longer change much with each additional step that you take.</p><h3 id="Gradient-Descent-Intuition"><a href="#Gradient-Descent-Intuition" class="headerlink" title="Gradient Descent Intuition"></a>Gradient Descent Intuition</h3><p><img src="/images/Gradient_Descent.png" alt="pic" title="Gradient Descent Intuition"></p><h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><p>If α is too small, Gradient descent may be slow.<br>If α is too large, Gradient descent may:</p><ul><li>Overshoot, never reach minimum</li><li>Fail to converge, diverge<br>Gradient Descent can reach local minimum with fixed learning rate.<br>Near a local minimum,</li><li>Derivative becomes smaller</li><li>Update steps become smaller</li></ul><h3 id="Gradient-Descent-for-Linear-Regression"><a href="#Gradient-Descent-for-Linear-Regression" class="headerlink" title="Gradient Descent for Linear Regression"></a>Gradient Descent for Linear Regression</h3><p><img src="/images/Gradient_Descent_for_Linear_Regression.png" alt="pic" title="Gradient Descent for Linear Regression"><br>“Batch” gradient descent: Each step of gradient descent uses all the training example.</p><h2 id="Linear-Regression-with-Multiple-Variables"><a href="#Linear-Regression-with-Multiple-Variables" class="headerlink" title="Linear Regression with Multiple Variables"></a>Linear Regression with Multiple Variables</h2><p><img src="/images/Multiple_Variables.png" alt="pic" title="Multiple Variables"><br>Models:</p><ul><li>Previously: f(x)&#x3D;wx+b</li><li>Multiple linear regression: f(x)&#x3D; w_1<em>x_1+w_2</em>x_2+w_3<em>x_3+w_4</em>x_4+b<br><img src="/images/Multiple_linear_regression.png" alt="pic" title="Multiple linear regression"></li></ul><h3 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h3><p><img src="/images/Vectorization.png%22" alt="pic" title="Vectorization"> </p><h3 id="Gradient-Descent-for-Multiple-Regression"><a href="#Gradient-Descent-for-Multiple-Regression" class="headerlink" title="Gradient Descent for Multiple Regression"></a>Gradient Descent for Multiple Regression</h3><p><img src="/images/Gradient_Descent_for_Linear_Regression.png" alt="pic" title="Gradient Descent for Multiple Regressionctorization"><br>Normal equation: A alternative to gradient descent.</p><ul><li>Only for linear regression</li><li>Solve for w,b without iterations<br>Disadvantages:</li><li>Doesn’t generalize to other learing algorithms.</li><li>Slow when number of features is large.</li></ul><h2 id="Feature-Scaling"><a href="#Feature-Scaling" class="headerlink" title="Feature Scaling"></a>Feature Scaling</h2><p><img src="/images/Feature_Scaling.png" alt="pic" title="Feature Scaling"></p><ul><li>Mean normalization</li><li>Z-score normalization</li></ul><h3 id="Checking-Gradient-Descent-for-Converge"><a href="#Checking-Gradient-Descent-for-Converge" class="headerlink" title="Checking Gradient Descent for Converge"></a>Checking Gradient Descent for Converge</h3><ul><li>Learning Curve</li><li>Automatic converge test: let epsilon be 0.001. If J(w,b) decreses by ≤ epsilon in one iteration, declare convergence.</li></ul><h3 id="Choosiong-the-Learning-Rate"><a href="#Choosiong-the-Learning-Rate" class="headerlink" title="Choosiong the Learning Rate"></a>Choosiong the Learning Rate</h3><p>With a small enough α, J(w,b) should decrease on every iteration.</p><h2 id="Feature-Engineering"><a href="#Feature-Engineering" class="headerlink" title="Feature Engineering"></a>Feature Engineering</h2><p>Feature Engineering: Using intuition to design new features, by transforming or combining original features.</p><h2 id="Polynomial-Regression"><a href="#Polynomial-Regression" class="headerlink" title="Polynomial Regression"></a>Polynomial Regression</h2><p>Features: x, x squared, x cubed…(Scikit-learn)</p><h1 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h1><ul><li>Negative class</li><li>Positive class</li></ul><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p>Sigmoid function:logistic function<br><img src="/images/Sigmoid_Function.png" alt="pic" title="Sigmoid Function"></p><h3 id="Decision-Boundary"><a href="#Decision-Boundary" class="headerlink" title="Decision Boundary"></a>Decision Boundary</h3><p><img src="/images/Sigmoid_Function_for_Mulitiple_Regression.png" alt="pic" title="Sigmoid Function for Mulitiple Regression"></p><h3 id="Cost-Function-for-Logistic-Regression"><a href="#Cost-Function-for-Logistic-Regression" class="headerlink" title="Cost Function for Logistic Regression"></a>Cost Function for Logistic Regression</h3><p>Squared error cost:</p><ul><li>Linear regression: convex cost function</li><li>Logistic regression: non-convex function<br><img src="/images/Square_error_cost.png" alt="pic" title="Square error cost"><br>Cost Function: average loss over all training samples.<br>Loss Function: predicts the target for single sample </li><li>Target: Measuring the Quality of Predicted Probabilities<br>In a binary classification problem, the model outputs a probability value, which represents the probability that a given sample belongs to class 1.</li><li>Maximum Likelihood Estimation</li><li>Taking the Log to Obtain the Log-Likelihood</li><li>Binary Cross-Entropy Loss<br><img src="/images/Cost_Function_for_Logistic_Regression.png" alt="pic" title="Cost Function for Logistic Regression"><br>When y(i)&#x3D;1:if the model predicts a high probility for class 1, the loss is small; otherwise, the loss is large. When y(i)&#x3D;0, if the model predicts a low probability for class 1 (i.e., a high probability for class 0), the loss is small; otherwise, the loss is large.</li></ul><h3 id="Simplified-Cost-Function"><a href="#Simplified-Cost-Function" class="headerlink" title="Simplified Cost Function"></a>Simplified Cost Function</h3><p><img src="/images/Simplified_Cost_Function.png" alt="pic" title="Simplified Cost Function"></p><h3 id="Gradient-Desctent-Implementation"><a href="#Gradient-Desctent-Implementation" class="headerlink" title="Gradient Desctent Implementation"></a>Gradient Desctent Implementation</h3><p>Goal: Find w,b, try to estimate the probability that the label Y is 1.<br>Same concepts:</p><ul><li>Minitor gradient descent(learning curve)</li><li>Vectorized implementation<br><img src="/images/Gradient_Desctent_Implementation_for_Logistic_Regression.png" alt="pic" title="Gradient Desctent Implementation for Logistic Regression"><br><img src="/images/Difference_between_Logistic_Regression_and_Linear_Regression.png" alt="pic" title="Difference between Logistic Regression and Linear Regression"></li></ul><h2 id="Regularization-to-Reduce-Overfitting"><a href="#Regularization-to-Reduce-Overfitting" class="headerlink" title="Regularization to Reduce Overfitting"></a>Regularization to Reduce Overfitting</h2><h3 id="The-problem-of-overfitting"><a href="#The-problem-of-overfitting" class="headerlink" title="The problem of overfitting"></a>The problem of overfitting</h3><p><img src="/images/Fitting.png" alt="pic" title="Three Regression Examples"></p><ul><li>Does not fit the training set well: high bias–&gt;underfit</li><li>Fits training set pretty well: generalization</li><li>Fits the training set extremely well: high variance–&gt;overfit</li></ul><h3 id="Addresssing-Overfitting"><a href="#Addresssing-Overfitting" class="headerlink" title="Addresssing Overfitting"></a>Addresssing Overfitting</h3><ul><li>Collect more training data</li><li>Select features to include&#x2F;exclude</li><li>Regularization: encourage the learning algorithm to shrink the values of parameters without neccessarily demanding that the parameter is set to exactly zero.</li></ul><h4 id="Cost-Function-with-Regularization"><a href="#Cost-Function-with-Regularization" class="headerlink" title="Cost Function with Regularization"></a>Cost Function with Regularization</h4><p>Lambda: regularization parameter<br>If lambda is too enormous–&gt;underfit<br>If lambda is too small–&gt;overfit<br><img src="/images/Regularization.png" alt="pic" title="Cost Function with Regularization"></p><h4 id="Regularized-Linear-Regression"><a href="#Regularized-Linear-Regression" class="headerlink" title="Regularized Linear Regression"></a>Regularized Linear Regression</h4><p><img src="/images/Regularized_Linear_Regression.png" alt="pic" title="Cost Function with Regularization"></p><h4 id="Regularuzed-Logistic-Regression"><a href="#Regularuzed-Logistic-Regression" class="headerlink" title="Regularuzed Logistic Regression"></a>Regularuzed Logistic Regression</h4><p><img src="/images/Regularized_Logistic_Regression.png" alt="pic" title="Regularuzed Logistic Regression"></p><h1 id="Advanced-learning-algorithms"><a href="#Advanced-learning-algorithms" class="headerlink" title="Advanced learning algorithms"></a>Advanced learning algorithms</h1><h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><p>Simplified mathematical model of neuron: inputs–&gt;neuron–&gt;outputs</p><h3 id="inference-prediction"><a href="#inference-prediction" class="headerlink" title="inference(prediction)"></a>inference(prediction)</h3><p><img src="/images/Demand_Prediction.png" alt="pic" title="Demand Prediction"></p><ul><li>Layer: A group of neurons which take as input the same or similiar features and that in turn output a few numbers together.</li><li>Affordability, awareness, perceived quality–&gt;<strong>activations</strong><br><img src="/images/Layer.png" alt="pic" title="Layer"></li><li>input layer: 4 numbers(vector)–&gt;hidden layer: 3 numbers–&gt;ouput layer: 1 numbers(vector)</li><li>Why called hidden layer: The data set tells you what is x and what is y, so you get data that tells you what are the correct inputs and the correct outputs, but the data set doesn’t tell you what are the correct values for affordability, awareness, perceived quality, so the correct data is hidden in the training set.</li><li>Cover up the left half of this diagram: a  logistic regression algorithm that is taking as input affordability, awareness, perceived quality of a t-shirt and using these three features to estimate the probability of the t-shirt being a top seller. What the neural network does is instead of you needing  to manually engineer the features, it can learn.</li><li>Multiple hidden layers(Multilayer perception): neural network architecture<br><img src="/images/Multiple_hidden_layers.png" alt="pic" title="Multiple hidden layers"></li></ul><h4 id="Example-Recognizing-Images"><a href="#Example-Recognizing-Images" class="headerlink" title="Example: Recognizing Images"></a>Example: Recognizing Images</h4><ul><li>Goal: Train a network with a million pixel brightness values, and outputs the indentity of the person in the picture.</li><li>Hidden layer: the 1st:looking for a little vertical line or edge; a oriented line; a line that orientation…the 2nd: group together; the 3rd: face </li><li>A remarkable feature: the neural network can learn these features detectors at the hidden layers all by itself.</li></ul><h4 id="Neural-network-layer"><a href="#Neural-network-layer" class="headerlink" title="Neural network layer"></a>Neural network layer</h4><p><img src="/images/Neural_network_layer.png" alt="pic" title="Neural network layer"> </p><h4 id="More-complex-neural-networks"><a href="#More-complex-neural-networks" class="headerlink" title="More complex neural networks"></a>More complex neural networks</h4><p><img src="/images/general_form.png" alt="pic" title="General Form"><br>j: jth neuron<br>j: jth neuron<br>[l]: lth layer<br>g: sigmoid function–&gt;<strong>activation function</strong><br>Each unit is a single neuron in the layer</p><h4 id="Inference-making-predictions-forward-propagation"><a href="#Inference-making-predictions-forward-propagation" class="headerlink" title="Inference: making predictions(forward propagation)"></a>Inference: making predictions(forward propagation)</h4><p>A handwritten digit recognition sample:<br>Forward  propagation: computation goes from left to right, propagating the activations of the neurons. These computations in the forward direction and this is in constrast to a different algorithm called backword propagation or back propagation.</p><h4 id="Building-the-model-using-TensorFlow"><a href="#Building-the-model-using-TensorFlow" class="headerlink" title="Building the model using TensorFlow"></a>Building the model using TensorFlow</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">x=np.array([[<span class="hljs-number">200.0</span>,<span class="hljs-number">17.0</span>]])<br>layer_1= Dense(units=<span class="hljs-number">3</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>a1= layer_1(x)<span class="hljs-comment">#1*3 matrix</span><br></code></pre></td></tr></table></figure><ul><li>Data in TensorFlow:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">x=np.array([[<span class="hljs-number">200</span>,<span class="hljs-number">17</span>]])<span class="hljs-comment">#[200,17] 1*2</span><br>x=np.array([[<span class="hljs-number">200</span>],<br>            [<span class="hljs-number">17</span>]<br>               ]) <span class="hljs-comment">#2*1--&gt;TensorFloW</span><br>x=np.array([<span class="hljs-number">200</span>,<span class="hljs-number">17</span>])<span class="hljs-comment">#1d vector, a linear array with no rows or no columns.--&gt; Linear Regression, Logistic Regression</span><br>tf.Tensor([[<span class="hljs-number">0.2</span> <span class="hljs-number">0.7</span> <span class="hljs-number">0.3</span>]], shape=(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>), dtype=float32)<br><br>layer_2= Dense(units=<span class="hljs-number">3</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>a2= layer_2(a1)<br>tf.Tensor([[<span class="hljs-number">0.8</span>]], shape=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), dtype=float32)<br>a2.numpy()<br></code></pre></td></tr></table></figure><h4 id="Building-a-neural-network"><a href="#Building-a-neural-network" class="headerlink" title="Building a neural network"></a>Building a neural network</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">layer_1= Dense(units=<span class="hljs-number">3</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>layer_2= Dense(units=<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>model = Sequential([layer_1, layer_2])<br>x = np.array([[<span class="hljs-number">200</span>,<span class="hljs-number">17</span>],<br>              [<span class="hljs-number">120</span>,<span class="hljs-number">5</span>]<br>              [<span class="hljs-number">425</span>,<span class="hljs-number">20</span>] <br>              [<span class="hljs-number">212</span>,<span class="hljs-number">18</span>]])<br>y = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>])<br>model.<span class="hljs-built_in">compile</span>(...)<br>model.fit(x,y)<br>model.predict(x_new)<span class="hljs-comment">#ouput a2</span><br><br>model = Sequential([<br>    Dense(units=<span class="hljs-number">3</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>    Dense(units=<span class="hljs-number">1</span> activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)])   <br></code></pre></td></tr></table></figure><h4 id="Forword-prop-in-a-single-layer"><a href="#Forword-prop-in-a-single-layer" class="headerlink" title="Forword prop in a single layer"></a>Forword prop in a single layer</h4><p><img src="/images/Forword_Prop.png" alt="pic" title="Forword Prop"> </p><h4 id="General-implemention-of-forward-propagation"><a href="#General-implemention-of-forward-propagation" class="headerlink" title="General implemention of forward propagation"></a>General implemention of forward propagation</h4><p><img src="/images/General_implemention_of_forward_propagation.png" alt="pic" title="General implemention of forward propagation"></p><h4 id="Is-there-a-path-to-AGI"><a href="#Is-there-a-path-to-AGI" class="headerlink" title="Is there a path to AGI"></a>Is there a path to AGI</h4><ul><li>ANI：artificial narrow intelligence</li><li>AGI: artificial general intelligence</li></ul><h4 id="Vectorization-1"><a href="#Vectorization-1" class="headerlink" title="Vectorization"></a>Vectorization</h4><p><img src="/images/Loops_vs_Vetetorization.png" alt="pic" title="Loops vs Vetetorization"> </p><h5 id="Matrix-multiplication"><a href="#Matrix-multiplication" class="headerlink" title="Matrix multiplication"></a>Matrix multiplication</h5><p><img src="/images/Matrix_multiplication_in_numpy.png" alt="pic" title="Matrix multiplication in Numpy"><br><img src="/images/Matrix_multiplication_in_tensorflow.png" alt="pic" title="Matrix Multiplication in Tensorflow"></p><h3 id="Net-Network-Training"><a href="#Net-Network-Training" class="headerlink" title="Net Network Training"></a>Net Network Training</h3><h4 id="Training-Details"><a href="#Training-Details" class="headerlink" title="Training Details"></a>Training Details</h4><ul><li>specify how to compute output given input x and parameters w,b(define model)–&gt;f(x)&#x3D;?</li><li>specify loss and cost </li><li>train on data to minimiaze J(w,b)<br><img src="/images/Model_Training_Steps.png" alt="pic" title="Model Training Steps"></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf <br><span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> Sequential<br><span class="hljs-keyword">from</span> tensorflow,keras <span class="hljs-keyword">import</span> Dense<br>    model = Sequential([<br>    Dense(units=<span class="hljs-number">25</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>),<br>    Dense(units=<span class="hljs-number">15</span> activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>),<br>    Dense(units=<span class="hljs-number">1</span> activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>),<br>                      ]) <br><span class="hljs-keyword">from</span> tensorflow.keras.losses <span class="hljs-keyword">import</span><br>BinaryCrossentropy<br>    model.<span class="hljs-built_in">compile</span>(loss=BinaryCrossentropy())<br><br>    model.fit(X,Y.epochs=<span class="hljs-number">100</span>)<span class="hljs-comment">#number of steps in gradient descent--&gt;compute derivatives for gradient descent using &quot;back propagation&quot;</span><br></code></pre></td></tr></table></figure><h4 id="Alternatives-to-the-sigmoid-activation"><a href="#Alternatives-to-the-sigmoid-activation" class="headerlink" title="Alternatives to the sigmoid activation"></a>Alternatives to the sigmoid activation</h4><p>ReLU: g(z)&#x3D;max(0,z)<br>Linear activation function: g(z)&#x3D;z</p><h4 id="Choosing-activation-function"><a href="#Choosing-activation-function" class="headerlink" title="Choosing activation function"></a>Choosing activation function</h4><ul><li>Output Layer:<br>Binary classification: Sigmoid<br>Regression: Linear(can be negative or positive)<br>Regression: ReLU(only positive)</li><li>Hidden Layer:<br>Most common choice: ReLU –&gt;faster<br>Binary classification: Sigmoid–&gt;flat, slow down learning</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tensorflow,keras <span class="hljs-keyword">import</span> Dense<br>model = Sequential([<br>    Dense(units=<span class="hljs-number">25</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    Dense(units=<span class="hljs-number">15</span> activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>    Dense(units=<span class="hljs-number">1</span> activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>),<br>                      ]) <br></code></pre></td></tr></table></figure><h4 id="Why-do-we-need-activation-functions"><a href="#Why-do-we-need-activation-functions" class="headerlink" title="Why do we need activation functions?"></a>Why do we need activation functions?</h4><p>Don’t use linear activations in hidden layers</p><h3 id="Multiclass"><a href="#Multiclass" class="headerlink" title="Multiclass"></a>Multiclass</h3><h4 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h4><p>Multiclass classification problem: target y can take on more than two possible values</p><ul><li>Softmax:<br><img src="/images/Softmax.png" alt="pic" title="Softmax"></li><li>Cost:<br><img src="/images/Cost.png" alt="pic" title="Cost"></li></ul><h4 id="Neural-Network-with-Softmax-output"><a href="#Neural-Network-with-Softmax-output" class="headerlink" title="Neural Network with Softmax output"></a>Neural Network with Softmax output</h4><p><img src="/images/Neural_Network_with_Softmax_output.png" alt="pic" title="Neural Network with Softmax output"></p><ul><li>Property: Each of these activation values depends onn all of the values of z.</li><li>Sparse: each digit is th e only one of these catagories.<br><img src="/images/MNIST.png" alt="pic" title="MNIST with softmax"></li></ul><h4 id="Improved-implementation-of-softmax"><a href="#Improved-implementation-of-softmax" class="headerlink" title="Improved implementation of softmax"></a>Improved implementation of softmax</h4><ul><li>Logistic regression:<br><img src="/images/Improved_implementation_of_Logistic_regression.png" alt="pic" title="Improved implementation of Logistic regression"></li><li>Softmax regression:<br><img src="/images/Improved_implementation_of_Softmax_regression.png" alt="pic" title="Improved implementation of Softmax regression"></li><li>Output layer use linear activation function: output z1~z10</li></ul><h4 id="Classification-with-multiple-outputs"><a href="#Classification-with-multiple-outputs" class="headerlink" title="Classification with multiple outputs"></a>Classification with multiple outputs</h4><p><img src="/images/Multilabel_Classification.png" alt="pic" title="Multilabel Classification"></p><h3 id="Advanced-Optimization"><a href="#Advanced-Optimization" class="headerlink" title="Advanced Optimization"></a>Advanced Optimization</h3><ul><li>Adam algotithm: adjust α automatically<br><img src="/images/Adam.png" alt="pic" title="MNIST Adam "></li></ul><h3 id="Addictional-Layer-Type"><a href="#Addictional-Layer-Type" class="headerlink" title="Addictional Layer Type"></a>Addictional Layer Type</h3><ul><li>Dense layer: every neuron in a layer gets as its inputs all the activations from the previous layer</li><li>Convolutional layer: Each neuron only looks at part of the previous layer’s outputs.<br>Why?</li><li>Faster computation</li><li>Need less training data(less prone to overfitting)<br><img src="/images/Convolutional_Neural_Network.png" alt="pic" title="Convolutional Neural Network"></li></ul><h2 id="Pratical-advice-for-building-machine-learning-systems"><a href="#Pratical-advice-for-building-machine-learning-systems" class="headerlink" title="Pratical advice for building machine learning systems"></a>Pratical advice for building machine learning systems</h2><h3 id="Debugging-a-learning-algorithm"><a href="#Debugging-a-learning-algorithm" class="headerlink" title="Debugging a learning algorithm"></a>Debugging a learning algorithm</h3><p>When it makes unacceptably large errors in predictions.</p><ul><li>Get more training examples</li><li>Try smaller sets of features </li><li>Try getting additional features</li><li>Try adding polynomial features</li><li>Try decreasing lambda </li><li>Try increasing lambda<br>Diagnostic: A test that you run to gain insight into what is&#x2F;isn’t working with a learning algorithm, to gain guidance into improving its performance.</li></ul><h3 id="Evaluating-a-model"><a href="#Evaluating-a-model" class="headerlink" title="Evaluating a model"></a>Evaluating a model</h3><ul><li>Training Set</li><li>Test Set<br><img src="/images/Train/test_procedure_for_linear_regression.png" alt="pic" title="Train&#x2F;test procedure for linear regression(with squared error cost"><br><img src="/images/Train/test_procedure_for_classification_problem.png" alt="pic" title="Train&#x2F;test procedure for classification problem(with squared error cost"></li><li>fraction of the test set and the fraction of  the train set that the algorithm has misclassified–&gt;count y_hat ! &#x3D; y</li><li>J_test(w,b): the fraction of the test set that has been misclassified</li><li>J_train(w,b): the fraction of the train set that has been misclassified</li></ul><h4 id="Model-selection-and-training-cross-validation-test-sets"><a href="#Model-selection-and-training-cross-validation-test-sets" class="headerlink" title="Model selection and training&#x2F;cross validation&#x2F;test sets"></a>Model selection and training&#x2F;cross validation&#x2F;test sets</h4><p><img src="/images/Train/Model_Selection.png" alt="pic" title="Train&#x2F;Model Selection"><br><img src="/images/Train/Cross_validation.png" alt="pic" title="Training&#x2F;cross validation&#x2F;test set"></p><h4 id="Diagnosing-bias-and-variance"><a href="#Diagnosing-bias-and-variance" class="headerlink" title="Diagnosing bias and variance"></a>Diagnosing bias and variance</h4><h2 id="Decision-Trees"><a href="#Decision-Trees" class="headerlink" title="Decision Trees"></a>Decision Trees</h2>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
