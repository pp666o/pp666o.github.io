---
title: Machine_learning
---
# Overview
## Definition
Field of study that gives computers the ability to learn without being explicitly programmed.
## Machine learning algorithms
- Supervised learning
- Unsupervised learning 
- Recommender systems
- Reinforcement learning
### Supervised Learning
- Regression
- Classification
Key charateristic:input-->output
**Regression**:Predicts a number from infinitely many possible numbers.
**Classification**:Predicts catagories from small number of possible outputs.Categories don't have to be numbers.
When it comes to two or more inputs:The learning algorithm has to decide how to fit a boundary line to this data.
### Unsupervised learning
Find something interesting in unlabeld data.
Key charateristc:Data only comes with inputs x,but not output labels y.Algorithm has to find structure in the data.
- Clustering
- Anomaly detection
- Dimensionality reduction
**Clustering**: Places the unlabeled data into different clusters,automatically finding stucture into data and figuring out what are the major types of individuals.
**Anomaly detection**:Finds unusual data points.
**Dimensionality reduction**：Compress data using fewer numbers.
--------
## Jupyter Notebooks

# Linear Regression
Training Set-->learing algorithms-->function:model
Linear Regressionin with one variable:y=wx+b
## Cost Function
f(x)=wx+b w,b:parameters/coeffients/weights
![pic](images/Cost_Function.png "Cost Function")
Cost Function:Square error cost function
Goal:Minimize J(w,b)
![pic](images/The_function_of_w.png "The function of w")
![pic](iamges/J(w,b).png "J(w,b) of Linear Function")
## Gradient Descent
Outline:
- Start with some w,b
- Keep changing w,b to reduce J(w,b)
- Until we settle at or near a minimum
### Implement Gradient Descent
![pic](iamges/Gradient_Descent_Algorithm.png "Gradient Descent Algorithm")
α：Learing Rate(between 0~1),controls how big of step you take downhill.
Derivative:Decide the direction to take step.
Repeat until convergence-->reach the point at a local
minimum where the parameters w and b no longer change much with each additional step that you take.
### Gradient Descent Intuition






