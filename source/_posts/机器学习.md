---
title: Machine_learning
---
# Overview
## Definition
Field of study that gives computers the ability to learn without being explicitly programmed.
## Machine learning algorithms
- Supervised learning
- Unsupervised learning 
- Recommender systems
- Reinforcement learnin
### Supervised Learning
- Regression
- Classification
Key charateristic:input-->output
**Regression**:Predicts a number from infinitely many possible numbers.
**Classification**:Predicts catagories from small number of possible outputs.Categories don't have to be numbers.
When it comes to two or more inputs:The learning algorithm has to decide how to fit a boundary line to this data.
### Unsupervised learning
Find something interesting in unlabeld data.
Key charateristc:Data only comes with inputs x,but not output labels y.Algorithm has to find structure in the data.
- Clustering
- Anomaly detection
- Dimensionality reduction
**Clustering**: Places the unlabeled data into different clusters,automatically finding stucture into data and figuring out what are the major types of individuals.
**Anomaly detection**:Finds unusual data points.
**Dimensionality reduction**：Compress data using fewer numbers.
--------
## Jupyter Notebooks
# Linear Regression
Training Set-->learing algorithms-->function:model
Linear Regressionin with one variable:y=wx+b
## Cost Function
f(x)=wx+b w,b:parameters/coeffients/weights
![pic](images/Cost_Function.png "Cost Function")
Cost Function:Square error cost function
Goal:Minimize J(w,b)
![pic](images/The_function_of_w.png "The function of w")
![pic](images/J(w,b).png "J(w,b) of Linear Function")
## Gradient Descent
Outline:
- Start with some w,b
- Keep changing w,b to reduce J(w,b)
- Until we settle at or near a minimum
### Implement Gradient Descent
![pic](images/Gradient_Descent_Algorithm.png "Gradient Descent Algorithm")
α：Learing Rate(between 0~1),controls how big of step you take downhill.
Derivative:Decide the direction to take step.
Repeat until convergence-->reach the point at a local
minimum where the parameters w and b no longer change much with each additional step that you take.
### Gradient Descent Intuition
![pic](images/Gradient_Descent.png "Gradient Descent Intuition")
### Learning Rate
If α is too small, Gradient descent may be slow.
If α is too large, Gradient descent may:
- Overshoot, never reach minimum
- Fail to converge, diverge
Gradient Descent can reach local minimum with fixed learning rate.
Near a local minimum,
- Derivative becomes smaller
- Update steps become smaller
### Gradient Descent for Linear Regression
![pic](images/Gradient_Descent_for_Linear_Regression.png "Gradient Descent for Linear Regression")
"Batch" gradient descent: Each step of gradient descent uses all the training example.
## Linear Regression with Multiple Variables
![pic](images/Multiple_Variables.png "Multiple Variables") 
Models:
- Previously: f(x)=wx+b
- Multiple linear regression: f(x)= w_1*x_1+w_2*x_2+w_3*x_3+w_4*x_4+b
![pic](images/Multiple_linear_regression.png "Multiple linear regression") 
### Vectorization
![pic](images/Vectorization.png" "Vectorization") 
### Gradient Descent for Multiple Regression
![pic](images/Gradient_Descent_for_Linear_Regression.png "Gradient Descent for Multiple Regressionctorization") 
Normal equation: A alternative to gradient descent.
- Only for linear regression
- Solve for w,b without iterations
Disadvantages:
- Doesn't generalize to other learing algorithms.
- Slow when number of features is large.
## Feature Scaling
![pic](images/Feature_Scaling.png "Feature Scaling")
- Mean normalization
- Z-score normalization
### Checking Gradient Descent for Converge
- Learning Curve
- Automatic converge test: let epsilon be 0.001. If J(w,b) decreses by ≤ epsilon in one iteration, declare convergence.
### Choosiong the Learning Rate
With a small enough α, J(w,b) should decrease on every iteration.
## Feature Engineering
Feature Engineering: Using intuition to design new features, by transforming or combining original features.
## Polynomial Regression
Features: x, x squared, x cubed...(Scikit-learn)
# Classification
- Negative class
- Positive class
## Logistic Regression
Sigmoid function:logistic function
![pic](images/Sigmoid_Function.png "Sigmoid Function")
### Decision Boundary
![pic](images/Sigmoid_Function_for_Mulitiple_Regression.png "Sigmoid Function for Mulitiple Regression")
### Cost Function for Logistic Regression
Squared error cost:
- Linear regression: convex cost function
- Logistic regression: non-convex function
![pic](images/Square_error_cost.png "Square error cost")
Cost Function: average loss over all training samples.
Loss Function: predicts the target for single sample 
- Target: Measuring the Quality of Predicted Probabilities
In a binary classification problem, the model outputs a probability value, which represents the probability that a given sample belongs to class 1.
- Maximum Likelihood Estimation
- Taking the Log to Obtain the Log-Likelihood
- Binary Cross-Entropy Loss
![pic](images/Cost_Function_for_Logistic_Regression.png "Cost Function for Logistic Regression")
When y(i)=1:if the model predicts a high probility for class 1, the loss is small; otherwise, the loss is large. When y(i)=0, if the model predicts a low probability for class 1 (i.e., a high probability for class 0), the loss is small; otherwise, the loss is large.
### Simplified Cost Function
![pic](images/Simplified_Cost_Function.png "Simplified Cost Function")
### Gradient Desctent Implementation
Goal: Find w,b, try to estimate the probability that the label Y is 1.
Same concepts:
- Minitor gradient descent(learning curve)
- Vectorized implementation
![pic](images/Gradient_Desctent_Implementation_for_Logistic_Regression.png "Gradient Desctent Implementation for Logistic Regression")
![pic](images/Difference_between_Logistic_Regression_and_Linear_Regression.png "Difference between Logistic Regression and Linear Regression")
## Regularization to Reduce Overfitting
### The problem of overfitting
![pic](images/Fitting.png "Three Regression Examples")
- Does not fit the training set well: high bias-->underfit
- Fits training set pretty well: generalization
- Fits the training set extremely well: high variance-->overfit
### Addresssing Overfitting
- Collect more training data
- Select features to include/exclude
- Regularization: encourage the learning algorithm to shrink the values of parameters without neccessarily demanding that the parameter is set to exactly zero.
#### Cost Function with Regularization
Lambda: regularization parameter
If lambda is too enormous-->underfit
If lambda is too small-->overfit
![pic](images/Regularization.png "Cost Function with Regularization")
#### Regularized Linear Regression
![pic](images/Regularized_Linear_Regression.png "Cost Function with Regularization")
#### Regularuzed Logistic Regression
![pic](images/Regularized_Logistic_Regression.png "Regularuzed Logistic Regression")