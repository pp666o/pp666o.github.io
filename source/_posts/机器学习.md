---
title: Machine_learning
---
# Overview
## Definition
Field of study that gives computers the ability to learn without being explicitly programmed.
## Machine learning algorithms
- Supervised learning
- Unsupervised learning 
- Recommender systems
- Reinforcement learnin
### Supervised Learning
- Regression
- Classification
Key charateristic:input-->output
**Regression**:Predicts a number from infinitely many possible numbers.
**Classification**:Predicts catagories from small number of possible outputs.Categories don't have to be numbers.
When it comes to two or more inputs:The learning algorithm has to decide how to fit a boundary line to this data.
### Unsupervised learning
Find something interesting in unlabeld data.
Key charateristc:Data only comes with inputs x,but not output labels y.Algorithm has to find structure in the data.
- Clustering
- Anomaly detection
- Dimensionality reduction
**Clustering**: Places the unlabeled data into different clusters,automatically finding stucture into data and figuring out what are the major types of individuals.
**Anomaly detection**:Finds unusual data points.
**Dimensionality reduction**：Compress data using fewer numbers.
--------
## Jupyter Notebooks
# Linear Regression
Training Set-->learing algorithms-->function:model
Linear Regressionin with one variable:y=wx+b
## Cost Function
f(x)=wx+b w,b:parameters/coeffients/weights
![pic](images/Cost_Function.png "Cost Function")
Cost Function:Square error cost function
Goal:Minimize J(w,b)
![pic](images/The_function_of_w.png "The function of w")
![pic](images/J(w,b).png "J(w,b) of Linear Function")
## Gradient Descent
Outline:
- Start with some w,b
- Keep changing w,b to reduce J(w,b)
- Until we settle at or near a minimum
### Implement Gradient Descent
![pic](images/Gradient_Descent_Algorithm.png "Gradient Descent Algorithm")
α：Learing Rate(between 0~1),controls how big of step you take downhill.
Derivative:Decide the direction to take step.
Repeat until convergence-->reach the point at a local
minimum where the parameters w and b no longer change much with each additional step that you take.
### Gradient Descent Intuition
![pic](images/Gradient_Descent.png "Gradient Descent Intuition")
### Learning Rate
If α is too small, Gradient descent may be slow.
If α is too large, Gradient descent may:
- Overshoot, never reach minimum
- Fail to converge, diverge
Gradient Descent can reach local minimum with fixed learning rate.
Near a local minimum,
- Derivative becomes smaller
- Update steps become smaller
### Gradient Descent for Linear Regression
![pic](images/Gradient_Descent_for_Linear_Regression.png "Gradient Descent for Linear Regression")
"Batch" gradient descent:Each step of gradient descent uses all the training example.
## Linear Regression with Multiple Variables
![pic](images/Multiple_Variables.png "Multiple Variables") 
Models:
- Previously: f(x)=wx+b
- Multiple linear regression: f(x)= w_1*x_1+w_2*x_2+w_3*x_3+w_4*x_4+b
![pic](images/Multiple_linear_regression.png "Multiple linear regression") 
### Vectorization
![pic](images/Vectorization.png" "Vectorization") 
### Gradient Descent for Multiple Regression
![pic](images/Gradient_Descent_for_Linear_Regression.png "Gradient Descent for Multiple Regressionctorization") 
Normal equation: A alternative to gradient descent.
- Only for linear regression
- Solve for w,b without iterations
Disadvantages:
- Doesn't generalize to other learing algorithms.
- Slow when number of features is large.
## Feature Scaling
![pic](images/Feature_Scaling.png "Feature Scaling")
- Mean normalization
- Z-score normalization
### Checking Gradient Descent for Converge
- Learning Curve
- Automatic converge test: let epsilon be 0.001. If J(w,b) decreses by ≤ epsilon in one iteration, declare convergence.
### Choosiong the Learning Rate
With a small enough α, J(w,b) should decrease on every iteration.
## Feature Engineering
Feature Engineering: Using intuition to design new features, by transforming or combining original features.
## Polynomial Regression
Features: x, x squared, x cubed...(Scikit-learn)
# Classification
- Negative class
- Positive class
## Logistic Regression
Sigmoid function:logistic function
![pic](images/Sigmoid_Function.png "Sigmoid Function")
### Decision Boundary
![pic](images/Sigmoid_Function_for_Mulitiple_Regression.png "Sigmoid Function for Mulitiple Regression")
