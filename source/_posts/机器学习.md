---
title: Machine_learning
---
# Overview
## Definition
Field of study that gives computers the ability to learn without being explicitly programmed.
## Machine learning algorithms
- Supervised learning
- Unsupervised learning 
- Recommender systems
- Reinforcement learning
### Supervised Learning
- Regression
- Classification
Key charateristic:input-->output
**Regression**:Predicts a number from infinitely many possible numbers.
**Classification**:Predicts catagories from small number of possible outputs.Categories don't have to be numbers.
When it comes to two or more inputs:The learning algorithm has to decide how to fit a boundary line to this data.
### Unsupervised learning
Find something interesting in unlabeld data.
Key charateristc:Data only comes with inputs x,but not output labels y.Algorithm has to find structure in the data.
- Clustering
- Anomaly detection
- Dimensionality reduction
**Clustering**: Places the unlabeled data into different clusters,automatically finding stucture into data and figuring out what are the major types of individuals.
**Anomaly detection**:Finds unusual data points.
**Dimensionality reduction**：Compress data using fewer numbers.
--------
## Jupyter Notebooks
# Linear Regression
Training Set-->learing algorithms-->function:model
Linear Regressionin with one variable:y=wx+b
## Cost Function
f(x)=wx+b w,b:parameters/coeffients/weights
![pic](images/Cost_Function.png "Cost Function")
Cost Function:Square error cost function
Goal:Minimize J(w,b)
![pic](images/The_function_of_w.png "The function of w")
![pic](images/J(w,b).png "J(w,b) of Linear Function")
## Gradient Descent
Outline:
- Start with some w,b
- Keep changing w,b to reduce J(w,b)
- Until we settle at or near a minimum
### Implement Gradient Descent
![pic](images/Gradient_Descent_Algorithm.png "Gradient Descent Algorithm")
α：Learing Rate(between 0~1),controls how big of step you take downhill.
Derivative:Decide the direction to take step.
Repeat until convergence-->reach the point at a local
minimum where the parameters w and b no longer change much with each additional step that you take.
### Gradient Descent Intuition
![pic](images/Gradient_Descent.png "Gradient Descent Intuition")
### Learning Rate
If α is too small, Gradient descent may be slow.
If α is too large, Gradient descent may:
- Overshoot, never reach minimum
- Fail to converge, diverge
Gradient Descent can reach local minimum with fixed learning rate.
Near a local minimum,
- Derivative becomes smaller
- Update steps become smaller
### Gradient Descent for Linear Regression
![pic](images/Gradient_Descent_for_Linear_Regression.png "Gradient Descent for Linear Regression")
"Batch" gradient descent: Each step of gradient descent uses all the training example.
## Linear Regression with Multiple Variables
![pic](images/Multiple_Variables.png "Multiple Variables") 
Models:
- Previously: f(x)=wx+b
- Multiple linear regression: f(x)= w_1*x_1+w_2*x_2+w_3*x_3+w_4*x_4+b
![pic](images/Multiple_linear_regression.png "Multiple linear regression") 
### Vectorization
![pic](images/Vectorization.png" "Vectorization") 
### Gradient Descent for Multiple Regression
![pic](images/Gradient_Descent_for_Linear_Regression.png "Gradient Descent for Multiple Regressionctorization") 
Normal equation: A alternative to gradient descent.
- Only for linear regression
- Solve for w,b without iterations
Disadvantages:
- Doesn't generalize to other learing algorithms.
- Slow when number of features is large.
## Feature Scaling
![pic](images/Feature_Scaling.png "Feature Scaling")
- Mean normalization
- Z-score normalization
### Checking Gradient Descent for Converge
- Learning Curve
- Automatic converge test: let epsilon be 0.001. If J(w,b) decreses by ≤ epsilon in one iteration, declare convergence.
### Choosiong the Learning Rate
With a small enough α, J(w,b) should decrease on every iteration.
## Feature Engineering
Feature Engineering: Using intuition to design new features, by transforming or combining original features.
## Polynomial Regression
Features: x, x squared, x cubed...(Scikit-learn)
# Classification
- Negative class
- Positive class
## Logistic Regression
Sigmoid function:logistic function
![pic](images/Sigmoid_Function.png "Sigmoid Function")
### Decision Boundary
![pic](images/Sigmoid_Function_for_Mulitiple_Regression.png "Sigmoid Function for Mulitiple Regression")
### Cost Function for Logistic Regression
Squared error cost:
- Linear regression: convex cost function
- Logistic regression: non-convex function
![pic](images/Square_error_cost.png "Square error cost")
Cost Function: average loss over all training samples.
Loss Function: predicts the target for single sample 
- Target: Measuring the Quality of Predicted Probabilities
In a binary classification problem, the model outputs a probability value, which represents the probability that a given sample belongs to class 1.
- Maximum Likelihood Estimation
- Taking the Log to Obtain the Log-Likelihood
- Binary Cross-Entropy Loss
![pic](images/Cost_Function_for_Logistic_Regression.png "Cost Function for Logistic Regression")
When y(i)=1:if the model predicts a high probility for class 1, the loss is small; otherwise, the loss is large. When y(i)=0, if the model predicts a low probability for class 1 (i.e., a high probability for class 0), the loss is small; otherwise, the loss is large.
### Simplified Cost Function
![pic](images/Simplified_Cost_Function.png "Simplified Cost Function")
### Gradient Desctent Implementation
Goal: Find w,b, try to estimate the probability that the label Y is 1.
Same concepts:
- Minitor gradient descent(learning curve)
- Vectorized implementation
![pic](images/Gradient_Desctent_Implementation_for_Logistic_Regression.png "Gradient Desctent Implementation for Logistic Regression")
![pic](images/Difference_between_Logistic_Regression_and_Linear_Regression.png "Difference between Logistic Regression and Linear Regression")
## Regularization to Reduce Overfitting
### The problem of overfitting
![pic](images/Fitting.png "Three Regression Examples")
- Does not fit the training set well: high bias-->underfit
- Fits training set pretty well: generalization
- Fits the training set extremely well: high variance-->overfit
### Addresssing Overfitting
- Collect more training data
- Select features to include/exclude
- Regularization: encourage the learning algorithm to shrink the values of parameters without neccessarily demanding that the parameter is set to exactly zero.
#### Cost Function with Regularization
Lambda: regularization parameter
If lambda is too enormous-->underfit
If lambda is too small-->overfit
![pic](images/Regularization.png "Cost Function with Regularization")
#### Regularized Linear Regression
![pic](images/Regularized_Linear_Regression.png "Cost Function with Regularization")
#### Regularuzed Logistic Regression
![pic](images/Regularized_Logistic_Regression.png "Regularuzed Logistic Regression")
# Advanced learning algorithms
## Neural Network
Simplified mathematical model of neuron: inputs-->neuron-->outputs
### inference(prediction)
![pic](images/Demand_Prediction.png "Demand Prediction")
- Layer: A group of neurons which take as input the same or similiar features and that in turn output a few numbers together.
- Affordability, awareness, perceived quality-->**activations**
![pic](images/Layer.png "Layer")
- input layer: 4 numbers(vector)-->hidden layer: 3 numbers-->ouput layer: 1 numbers(vector)
- Why called hidden layer: The data set tells you what is x and what is y, so you get data that tells you what are the correct inputs and the correct outputs, but the data set doesn't tell you what are the correct values for affordability, awareness, perceived quality, so the correct data is hidden in the training set.
- Cover up the left half of this diagram: a  logistic regression algorithm that is taking as input affordability, awareness, perceived quality of a t-shirt and using these three features to estimate the probability of the t-shirt being a top seller. What the neural network does is instead of you needing  to manually engineer the features, it can learn.
- Multiple hidden layers(Multilayer perception): neural network architecture
![pic](images/Multiple_hidden_layers.png "Multiple hidden layers")
#### Example: Recognizing Images
- Goal: Train a network with a million pixel brightness values, and outputs the indentity of the person in the picture.
- Hidden layer: the 1st:looking for a little vertical line or edge; a oriented line; a line that orientation...the 2nd: group together; the 3rd: face 
- A remarkable feature: the neural network can learn these features detectors at the hidden layers all by itself.
#### Neural network layer
![pic](images/Neural_network_layer.png "Neural network layer") 
#### More complex neural networks
![pic](images/general_form.png "General Form") 
j: jth neuron
j: jth neuron  
[l]: lth layer 
g: sigmoid function-->**activation function**
Each unit is a single neuron in the layer
#### Inference: making predictions(forward propagation)
A handwritten digit recognition sample:
Forward  propagation: computation goes from left to right, propagating the activations of the neurons. These computations in the forward direction and this is in constrast to a different algorithm called backword propagation or back propagation.
#### Building the model using TensorFlow
```python
x=np.array([[200.0,17.0]])
layer_1= Dense(units=3, activation='sigmoid')
a1= layer_1(x)#1*3 matrix
```
- Data in TensorFlow: 
```python
x=np.array([[200,17]])#[200,17] 1*2
x=np.array([[200],
            [17]
               ]) #2*1-->TensorFloW
x=np.array([200,17])#1d vector, a linear array with no rows or no columns.--> Linear Regression, Logistic Regression
tf.Tensor([[0.2 0.7 0.3]], shape=(1,3), dtype=float32)

layer_2= Dense(units=3, activation='sigmoid')
a2= layer_2(a1)
tf.Tensor([[0.8]], shape=(1,1), dtype=float32)
a2.numpy()
```
#### Building a neural network
```python
layer_1= Dense(units=3, activation='sigmoid')
layer_2= Dense(units=1, activation='sigmoid')
model = Sequential([layer_1, layer_2])
x = np.array([[200,17],
              [120,5]
              [425,20] 
              [212,18]])
y = np.array([1,0,0,1])
model.compile(...)
model.fit(x,y)
model.predict(x_new)#ouput a2

model = Sequential([
    Dense(units=3, activation='sigmoid')
    Dense(units=1 activation='sigmoid')])   
```
#### Forword prop in a single layer
![pic](images/Forword_Prop.png "Forword Prop") 
#### General implemention of forward propagation
![pic](images/General_implemention_of_forward_propagation.png "General implemention of forward propagation")
#### Is there a path to AGI 
- ANI：artificial narrow intelligence
- AGI: artificial general intelligence
#### Vectorization
![pic](images/Loops_vs_Vetetorization.png "Loops vs Vetetorization") 
##### Matrix multiplication 
![pic](images/Matrix_multiplication_in_numpy.png "Matrix multiplication in Numpy") 
![pic](images/Matrix_multiplication_in_tensorflow.png "Matrix Multiplication in Tensorflow")
### Net Network Training
#### Training Details 
- specify how to compute output given input x and parameters w,b(define model)-->f(x)=?
- specify loss and cost 
- train on data to minimiaze J(w,b) 
![pic](images/Model_Training_Steps.png "Model Training Steps")
```python
import tensorflow as tf 
from tensorflow.keras import Sequential
from tensorflow,keras import Dense
    model = Sequential([
    Dense(units=25, activation='sigmoid'),
    Dense(units=15 activation='sigmoid'),
    Dense(units=1 activation='sigmoid'),
                      ]) 
from tensorflow.keras.losses import
BinaryCrossentropy
    model.compile(loss=BinaryCrossentropy())

    model.fit(X,Y.epochs=100)#number of steps in gradient descent-->compute derivatives for gradient descent using "back propagation"
```
#### Alternatives to the sigmoid activation
ReLU: g(z)=max(0,z)
Linear activation function: g(z)=z
#### Choosing activation function
- Output Layer: 
Binary classification: Sigmoid
Regression: Linear(can be negative or positive)
Regression: ReLU(only positive)
- Hidden Layer:
Most common choice: ReLU -->faster
Binary classification: Sigmoid-->flat, slow down learning 
```python
from tensorflow,keras import Dense
model = Sequential([
    Dense(units=25, activation='relu'),
    Dense(units=15 activation='relu'),
    Dense(units=1 activation='sigmoid'),
                      ]) 
```
#### Why do we need activation functions?
Don't use linear activations in hidden layers
### Multiclass 
#### Softmax
Multiclass classification problem: target y can take on more than two possible values
- Softmax: 
![pic](images/Softmax.png "Softmax")
- Cost: 
![pic](images/Cost.png "Cost")
#### Neural Network with Softmax output
![pic](images/Neural_Network_with_Softmax_output.png "Neural Network with Softmax output")
- Property: Each of these activation values depends onn all of the values of z.
- Sparse: each digit is th e only one of these catagories.
![pic](images/MNIST.png "MNIST with softmax")
#### Improved implementation of softmax
- Logistic regression:
![pic](images/Improved_implementation_of_Logistic_regression.png "Improved implementation of Logistic regression")
- Softmax regression:
![pic](images/Improved_implementation_of_Softmax_regression.png "Improved implementation of Softmax regression")
- Output layer use linear activation function: output z1~z10
#### Classification with multiple outputs
![pic](images/Multilabel_Classification.png "Multilabel Classification")
### Advanced Optimization
- Adam algotithm: adjust α automatically
![pic](images/Adam.png "MNIST Adam ")
### Addictional Layer Type
- Dense layer: every neuron in a layer gets as its inputs all the activations from the previous layer
- Convolutional layer: Each neuron only looks at part of the previous layer's outputs.
Why?
- Faster computation
- Need less training data(less prone to overfitting)
![pic](images/Convolutional_Neural_Network.png "Convolutional Neural Network")
## Pratical advice for building machine learning systems
### Debugging a learning algorithm
When it makes unacceptably large errors in predictions.
- Get more training examples
- Try smaller sets of features 
- Try getting additional features
- Try adding polynomial features
- Try decreasing lambda 
- Try increasing lambda
Diagnostic: A test that you run to gain insight into what is/isn't working with a learning algorithm, to gain guidance into improving its performance.
### Evaluating a model
- Training Set
- Test Set
![pic](images/Train/test_procedure_for_linear_regression.png "Train/test procedure for linear regression(with squared error cost")
![pic](images/Train/test_procedure_for_classification_problem.png "Train/test procedure for classification problem(with squared error cost")
- fraction of the test set and the fraction of  the train set that the algorithm has misclassified-->count y_hat ! = y
- J_test(w,b): the fraction of the test set that has been misclassified
- J_train(w,b): the fraction of the train set that has been misclassified
#### Model selection and training/cross validation/test sets 
![pic](images/Train/Model_Selection.png "Train/Model Selection")
![pic](images/Train/Cross_validation.png "Training/cross validation/test set")
#### Diagnosing bias and variance

## Decision Trees